{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](image/LambdaSchool.png)\n",
    "\n",
    "# Supervised Machine Learning with Numerical and Text-Based Features\n",
    "\n",
    "### Details\n",
    "* <b> Event: </b> Lambda School Guest Lecture\n",
    "* <b> Instructor: </b> Bruno Janota, Senior Data Scientist at Lockheed Martin\n",
    "* <b> Date: </b> Monday, June 17th, 2019\n",
    "\n",
    "### Sections\n",
    "1. [Representing Text as Numbers](#1.-Representing-Text-as-Numbers)\n",
    "2. [LDA Topic Modeling](#2.-LDA-Topic-Modeling)\n",
    "3. [Next Steps](#3.-Next-Steps)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Representing Text as Numbers\n",
    "\n",
    "In order for a computer to execute any form of analytics or machine learning on natural language, data scientists must convert the raw text that we as humans can comprehend into a format that computers can understand.  That format is a numeric representation.\n",
    "\n",
    "The first task in any Natural Language Processing analysis is to parse the raw text into objects called tokens. This process is called <b>tokenization</b>. The result of tokenization is a list of words that represents each text input (document, tweet, etc.) in a dataset. \n",
    "\n",
    "Let's tokenize an example dataset of generic sentences. Feel free to experiment with your own sentence. For this task, we will use a python package developed by NLP researchers at Stanford called `NLTK` which stands for Natural Language ToolKit."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the nltk package\n",
    "import nltk\n",
    "nltk.download(\"punkt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Experiment with your own sentence and observe the tokens\n",
    "sentence = \"I will be an NLP guru by the end of lunch today.\"\n",
    "tokens = nltk.word_tokenize(sentence)\n",
    "print(f\"'{sentence}' becomes\\n{tokens}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1 Term Frequency & Text Pre-Processing\n",
    "\n",
    "Once the raw text has been tokenized, one of the simplest ways to represent the tokenized data in numeric form is to compute the number of occurences of a word in the text, more commonly known as the <b>term frequency</b>.  This approach takes the tokens and counts the number of occurences for each within the tokenized text.\n",
    "\n",
    "Luckily, there are many pre-built options in Python for performing this task. We will use Scikit-Learn's `CountVectorizer` function to calculate the term frequencies. Scikit-Learn is one of the industry standard Python packages for analytics.\n",
    "\n",
    "Lets calculate the term frequency over a list of sentences. We can provide the list of raw text to the `CountVectorizer` function and use its default tokenizer, or pass the `NLTK` tokenizer as an argument."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "sentences = [\n",
    "    \"I like this movie, very funny\",\n",
    "    \"I hate all movies.\",\n",
    "    \"Movie was awesome! I loved it.\",\n",
    "    \"Good movie. I love it.\"\n",
    "]\n",
    "\n",
    "tf_vectorizer = CountVectorizer(lowercase=False)\n",
    "X = tf_vectorizer.fit_transform(sentences)\n",
    "\n",
    "tf_df = pd.DataFrame(data = X.toarray(), columns = tf_vectorizer.get_feature_names())\n",
    "print(f'Number of sentences: {tf_df.shape[0]}\\nNumber of tokens: {tf_df.shape[1]}\\n')\n",
    "tf_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This table shows us how term frequency works. Each row is a sentence in our `sentences` list, and the columns represent the unique word counts across the dataset.\n",
    "\n",
    "<i>Does anything odd stand out to you about the tokens themselves?</i>\n",
    "\n",
    "These nuances are common problems associated with Natural Language Processing tasks but luckily there are several well documented aproaches and general NLP best practices to these problems that we will discuss in more detail below:\n",
    "- Stop Word Removal\n",
    "- Lowercasing text\n",
    "- Stemming (rules based hueristic approach to convert words into their stem)\n",
    "- Lemmatization (utilizes pre-determined vocabulary and morphological analysis of words to return the base form of a word)\n",
    "\n",
    "Let's repeat the tokenization process above by using a different tokenizer than the default within `CountVectorizer`, converting tokens to lowercase, remove stop words, and observing the resulting term frequency table."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Stop Word Removal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pythons NaturaL Lanugage Tool Kit (NLTK) package contains a list of 179 commonly used english words that do not have much value in helping to understand or extract meaning from text. It is usually a good starting point but can be easily extended for text in specific domains (social media, emails, surveys, etc.). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# NLTK Stop words\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "stop_words = stopwords.words('english')\n",
    "#stop_words.extend(['from', 'subject', 're', 'edu', 'use'])\n",
    "\n",
    "print('Number of Stop Words: {}'.format(len(stop_words)))\n",
    "print('Example Stop Words: {}'.format(stop_words[0:20]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Experiment with your own sentence and observe the tokens\n",
    "sentence = \"I will be an NLP guru by the end of lunch today.\"\n",
    "tokens = nltk.word_tokenize(sentence)\n",
    "tokens_no_stop = [word for word in tokens if word not in stop_words]\n",
    "print(f\"Tokenized sentence: '{tokens}\")\n",
    "print(f\"Tokenized sentence without stop words: '{tokens_no_stop}'\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Lowercasing Text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's tokenize the same sentences as above removing stop words and lowercasing words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf_vectorizer = CountVectorizer(lowercase=True, stop_words=stop_words)\n",
    "X = tf_vectorizer.fit_transform(sentences)\n",
    "\n",
    "tf_df = pd.DataFrame(data = X.toarray(), columns = tf_vectorizer.get_feature_names())\n",
    "print(f'Number of sentences: {tf_df.shape[0]}\\nNumber of tokens: {tf_df.shape[1]}\\n')\n",
    "tf_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is great! We have reduced the sentences down to the most relevant words but there is still some duplication such as \"movie\" and \"movies\" and \"love\" and \"loved\"."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Lemmatization and Stemming"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nltk.download(\"wordnet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "\n",
    "# Experiment with your own sentence and observe the tokens\n",
    "sentence = \"The movies were awesome! I loved it.\" \n",
    "\n",
    "# Initialize Common Lemmatizers/Stemmers\n",
    "wnl = WordNetLemmatizer()\n",
    "ps = PorterStemmer()\n",
    "\n",
    "# Compare WordNetLemmatizer and PorterStemmer\n",
    "tokens = nltk.word_tokenize(sentence)\n",
    "tokens_wnl = [wnl.lemmatize(t) for t in nltk.word_tokenize(sentence)]\n",
    "tokens_ps = [ps.stem(t) for t in nltk.word_tokenize(sentence)]\n",
    "print(f\"Tokenized sentence: \\n{tokens}\")\n",
    "print(f\"Tokenized sentence with WordNetLemmatizer: \\n{tokens_wnl}'\")\n",
    "print(f\"Tokenized sentence with PorterStemmer: \\n{tokens_ps}'\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In addition to the methods discussed above there are also python functions to remove punctuation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary packages\n",
    "import string\n",
    "\n",
    "# Remove the punctuation from our sentences\n",
    "sentences_no_punc = [s.translate(str.maketrans('', '', string.punctuation)) for s in sentences]\n",
    "\n",
    "# Examine sentences_no_punc\n",
    "sentences_no_punc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Spell Check/Autocorrect"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lastly, depending on the dataset you may want to autocorrect the text to catch misspellings (social media or other user generated text data)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from autocorrect import spell\n",
    "\n",
    "word = 'mussage'\n",
    "print(f'Original Spelling: {word}')\n",
    "print(f'Autocorrect: {spell(word)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are many pre-built packages in python that can help you transform and pre-process text data. This section introduced you to several of the common best practice text cleaning options. When there doesnt exist a pre-built option, it is fairly straightforward to build your own!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2 Term Frequency - Inverse Document Frequency (TF-IDF)\n",
    "\n",
    "While term frequency is a great way to represent text in a numeric fashion, there are more advanced methods that capture information about the entire dataset. <b>Term Frequency-Inverse Document Frequency</b>, better known as <b>TF-IDF</b>, is another common, more advanced iteration of term frequency, and can be used to better represent text in relation to the entire dataset of text.\n",
    "\n",
    "TF-IDF uses a calculation to determine a term's importance within the entire dataset. In easy to understand language, if a term occurs frequently in observation of the dataset, referred to a document, and doesn't occur frequently in other documents in the dataset, it should be given a higher numeric value. It is given a higher numeric value because it is unique to the identification of the document that it exists in. Terms that are common across most or all of the documents in the dataset in turn are given lower numeric values, because they don't help distinguish one document from another.\n",
    "\n",
    "If you are interested in the math behind TF-IDF, it can be found [here](https://en.wikipedia.org/wiki/Tf%E2%80%93idf).\n",
    "\n",
    "Lets use the same simple example as above to better visualize this numeric representation. We will use the `LemmaTokenizer` we created above to perform the tokenization and lemmatization, with one additional feature; we will remove punctuation as well!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "class LemmaTokenizer(object):\n",
    "    def __init__(self):\n",
    "        self.wnl = WordNetLemmatizer()\n",
    "    def __call__(self, articles):\n",
    "        return [self.wnl.lemmatize(t) for t in nltk.word_tokenize(articles)]\n",
    "\n",
    "# Initialize TF-IDF\n",
    "tfidf_vectorizer = TfidfVectorizer(tokenizer=LemmaTokenizer(),\n",
    "                                   lowercase=True)\n",
    "\n",
    "# Fit to sentences_no_punc and transform into array\n",
    "X = tfidf_vectorizer.fit_transform(sentences_no_punc)\n",
    "\n",
    "tfidf_df = pd.DataFrame(data = X.toarray(), columns = tfidf_vectorizer.get_feature_names()).round(3)\n",
    "tfidf_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TF-IDF is the most common form of converting documents within a dataset, also known as corpus, into a numeric representation.  Now that we know how to preprocess raw text for modeling, lets learn about some different types of models and use some real-life data!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Topic Modeling\n",
    "\n",
    "A topic model is a type of statistical model for discovering the abstract \"topics\" that occur in a collection of documents. Topic modeling is a frequently used text-mining tool for discovery of hidden semantic structures in a text body. Intuitively, given that a document is about a particular topic, one would expect particular words to appear in the document more or less frequently: \"dog\" and \"bone\" will appear more often in documents about dogs, \"cat\" and \"meow\" will appear in documents about cats, and \"the\" and \"is\" will appear equally in both. A document typically concerns multiple topics in different proportions; thus, in a document that is 10% about cats and 90% about dogs, there would probably be about 9 times more dog words than cat words. The \"topics\" produced by topic modeling techniques are clusters of similar words. A topic model captures this intuition in a mathematical framework, which allows examining a set of documents and discovering, based on the statistics of the words in each, what the topics might be and what each document's balance of topics is."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load MLB Pitcher Data\n",
    "#### Career Statistics, Pitch Style Description and Tommy John Surgery Indicator\n",
    "\n",
    "In this section, we will explore a dataset that contains career level data and pitch style descriptions for 758 MLB starting pitchers that threw more than 100 innings between 2010-2018 as well as an indication of whether or not the pitcher got Ulnar Collateral Ligament Reconstruction commonly referred to as Tommy John Surgery. \n",
    "- The career statistics were exported from Fangraphs.com\n",
    "- Pitch style descriptions were scraped from BrooksBaseball.net player cards\n",
    "- TJ Surgery indicator was merged from google sheet maintained by @MLBPlayerAnalysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Read Career Statistics from FanGraphs and TJ Surgery Data with Pitch Style\n",
    "fg_df = pd.read_csv('./data/FanGraphsCareerData.csv')\n",
    "tj_df = pd.read_csv('./data/MLB_Pitchers_2008to2018_withBBPitchStyle.csv')\n",
    "print(f'FanGraphs Dataframe Shape: {fg_df.shape}')\n",
    "print(f'TJ Dataframe Shape: {tj_df.shape}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merge Final dataframe with TJ label and FanGraphs Stats\n",
    "df = pd.merge(tj_df, fg_df, on='playerid')\n",
    "print(f'Final Dataframe Shape: {df.shape}\\n')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see right away that some features like IP (Innings Pitched) have skewed distributions and we can also see that many of the features have different units (i.e. innings, time, etc.). Feature scaling may help improve our model performance. We will use the sklearn MinMax scaler to normalize all the numerical features between 0 and 1 or -1 and 1 (for features with negative values). \n",
    "\n",
    "Standard Scaler:\n",
    "$\n",
    "\\begin{align}\n",
    "\\frac{x_i - mean(x)} {stdev(x)}\n",
    "\\end{align}\n",
    "$\n",
    "\n",
    "Min-Max Scaler:\n",
    "$\n",
    "\\begin{align}\n",
    "\\frac{x_i - min(x)} {max(x)–min(x)}\n",
    "\\end{align}\n",
    "$\n",
    "\n",
    "Robust Scaler:\n",
    "$\n",
    "\\begin{align}\n",
    "\\frac{x_i - Q1(x)} {Q3(x)–Q1(x)}\n",
    "\\end{align}\n",
    "$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn import preprocessing\n",
    "\n",
    "# Find numeric columns\n",
    "num_cols = df.columns[df.dtypes.apply(lambda c: np.issubdtype(c, np.number))]\n",
    "\n",
    "# Scale the numeric columns from 0-1 with Standard Scaler\n",
    "scaler = preprocessing.RobustScaler()\n",
    "df[num_cols] = scaler.fit_transform(df[num_cols])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Review list of columns in our dataset\n",
    "print('Columns in our Dataset: \\n{}'.format(list(df.columns)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's look at the Pitch Style Description for Clayton Kershaw\n",
    "player = 'Clayton Kershaw'\n",
    "print('Player Name: \\n{}\\n'.format(player))\n",
    "print('Pitch Style Description: \\n{}'.format(list(df[df.Name_x == player][\"BB_PitchStyle\"])))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's look at how many pitchers in our dataset have gotten Tommy John surgery. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.tjSurgery.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TJ surgery as a percentage\n",
    "df.tjSurgery.value_counts(normalize=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check if the dataset has any missing data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "df.isna().sum().plot.bar(figsize=(16,6), color='blue')\n",
    "plt.title('Count of Missing Values per Column (n = 758)')\n",
    "plt.ylabel('Counts')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The missing data corresponds to velocity, percentage, and movement of pitches that a particular pitcher does not throw so we will fill with 0."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.fillna(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When performing analysis on text data it is always a good idea to check for duplicate entries for fields that should be unique identifiers (playerid) and dropping rows that do not have any text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check for duplicates in playerid\n",
    "print('Any Duplicate PlayerIDs? {}'.format(any(df['playerid'].duplicated())))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Let's transition to looking at pitch style description and some basic metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "document_lengths = np.array(list(map(len, df.BB_PitchStyle.str.split(' '))))\n",
    "\n",
    "print(\"The average number of words in a pitch style description is: {}.\".format(np.mean(document_lengths)))\n",
    "print(\"The minimum number of words in a pitch style description is: {}.\".format(min(document_lengths)))\n",
    "print(\"The maximum number of words in a pitch style description is: {}.\".format(max(document_lengths)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's take a look at the distribution of pitch style descriptions by whether or not a pitcher got Tommy John Surgery."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(10,6))\n",
    "\n",
    "sns.distplot(document_lengths[df.tjSurgery == 'Yes'], ax=ax, label='Injured')\n",
    "sns.distplot(document_lengths[df.tjSurgery == 'No'], ax=ax, label='Not Injured')\n",
    "\n",
    "ax.set_title(\"Distribution of Number of Words\", fontsize=16)\n",
    "ax.set_xlabel(\"Number of Words\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pre-Process Text and Build Vector Representation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Convert raw text to document-term matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to preprocess raw test from job description\n",
    "def lemmatize(text):\n",
    "    text = text.split()    \n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    lem_words = [lemmatizer.lemmatize(word) for word in text]\n",
    "    text = \" \".join(lem_words)\n",
    "    return text\n",
    "\n",
    "# Remove extra white space\n",
    "df['BB_PitchStyle_Clean'] = df['BB_PitchStyle'].apply(lambda x: ' '.join(x.split()))\n",
    "\n",
    "# Remove punctuation and numbers\n",
    "df['BB_PitchStyle_Clean'] = df['BB_PitchStyle_Clean'].str.replace('[^\\w\\s]', ' ').str.replace('\\d+', '')\n",
    "\n",
    "# Convert to lower case\n",
    "df['BB_PitchStyle_Clean'] = df['BB_PitchStyle_Clean'].str.lower()\n",
    "\n",
    "# Lemmatize\n",
    "df['BB_PitchStyle_Clean'] = df['BB_PitchStyle_Clean'].map(lambda x: lemmatize(x))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's look at the pitch style description before and after cleaning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "player = 'Clayton Kershaw'\n",
    "print('Original Text: \\n{}\\n'.format(list(df[df.Name_x == player][\"BB_PitchStyle\"])))\n",
    "print('After Cleaning: \\n{}'.format(list(df[df.Name_x == player][\"BB_PitchStyle_Clean\"])))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Let's build a Document Term Matrix for use in building our LDA topic model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "stop_words.extend(['mph','beta','feature'])\n",
    "tf_vectorizer = CountVectorizer(ngram_range = (1,2),\n",
    "                                stop_words = stop_words,\n",
    "                                max_df = 0.8, \n",
    "                                min_df = 2)\n",
    "\n",
    "dtm_tf = tf_vectorizer.fit_transform(df.BB_PitchStyle_Clean)\n",
    "dtm_feature_names = tf_vectorizer.get_feature_names()\n",
    "\n",
    "print('Document Term Matrix Shape: {}'.format(dtm_tf.shape))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Look at top 50 most frequent words in the pitch style descriptions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(16,6))\n",
    "term_df = pd.DataFrame(dtm_tf.toarray(), columns=dtm_feature_names)\n",
    "term_df.sum(axis=0).sort_values(ascending=False)[0:50].plot.bar(color='blue')\n",
    "plt.title(\"Top 50 Most Frequent Words in Pitch Style Descriptions\")\n",
    "plt.ylabel(\"Frequency\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Build the Topic Model\n",
    "\n",
    "We have everything required to train the LDA model. In addition to the document term matrix, you need to provide:\n",
    "- the number of topics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import LatentDirichletAllocation\n",
    "\n",
    "n_topics = 10\n",
    "lda_tf = LatentDirichletAllocation(n_components=n_topics, random_state=10)\n",
    "lda_tf.fit(dtm_tf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### View the topics with pyLDAvis\n",
    "\n",
    "The above LDA model is built with 10 topics where each topic is a combination of keywords and each keyword contributes a certain weight to the topic. You can see the keywords for each topic and the importance of each keyword using lda_model.print_topics() as shown next.\n",
    "\n",
    "![](images/Inferring-Topic-from-Keywords.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyLDAvis\n",
    "import pyLDAvis.sklearn\n",
    "pyLDAvis.enable_notebook()\n",
    "\n",
    "# Visualize the topics\n",
    "pyLDAvis.sklearn.prepare(lda_tf, dtm_tf, tf_vectorizer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Build a Simple Decision Tree"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Remove columns that should not be included in the models prediction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1 = df[df.columns.difference(['playerid','Name_x','BB_PitchStyle','Name_y','Team','IP','BB_PitchStyle_Clean','throws'])]\n",
    "print('Shape of New DataFrame: {}\\n'.format(df1.shape))\n",
    "print('Features to Include in Model: \\n{}'.format(list(df1.columns)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Split Full Data Set into Train/Validation Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.decomposition import LatentDirichletAllocation\n",
    "\n",
    "# Set seed for random number generator for reproducibility\n",
    "seed=5\n",
    "\n",
    "# Split dataframe into features and labels\n",
    "features = df1.loc[:, df1.columns != 'tjSurgery']\n",
    "labels = df1['tjSurgery']\n",
    "\n",
    "# Split data using 80% to train model and 20% to validate performance\n",
    "X_train1, X_test1, y_train1, y_test1 = train_test_split(features, labels, test_size = 0.2, random_state = seed)\n",
    "\n",
    "# Confirm Shape of Train/Test data\n",
    "print('Shape of Train Features: {}'.format(X_train1.shape))\n",
    "print('Shape of Train Labels:   {}'.format(y_train1.shape))\n",
    "print('Shape of Test Features:  {}'.format(X_test1.shape))\n",
    "print('Shape of Test Labels:    {}'.format(y_test1.shape))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Build a Decision Tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "# Train decision tree \n",
    "dtree1 = DecisionTreeClassifier(class_weight='balanced', random_state=seed)\n",
    "dtree1.fit(X_train1, y_train1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Get top_n Feature Importances from Decision Tree\n",
    "\n",
    "The importance of a feature in our Decision Tree is computed as the (normalized) total reduction of the criterion brought by that feature and is also known as the Gini importance. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "top_n = 10\n",
    "feat_imp1 = pd.DataFrame({'Importance': dtree1.feature_importances_})    \n",
    "feat_imp1['Feature'] = X_train1.columns\n",
    "feat_imp1.sort_values(by='Importance', ascending=False, inplace=True)\n",
    "feat_imp1 = feat_imp1.iloc[:top_n]\n",
    "\n",
    "# Plot Feature Importance Values\n",
    "plt.figure(figsize=(20,6))\n",
    "plt.bar(feat_imp1['Feature'], feat_imp1['Importance'])\n",
    "plt.xticks(rotation=45)\n",
    "plt.title('Model #1: Feature Importance')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import scikitplot as skplt\n",
    "\n",
    "# Make predictions on validation data\n",
    "test_predictions1 = dtree1.predict(X_test1)\n",
    "\n",
    "# Plot Confusion Matrix on Validation Data\n",
    "skplt.metrics.plot_confusion_matrix(y_test1, test_predictions1, \n",
    "                                    figsize=(8,8),\n",
    "                                    x_tick_rotation=90,\n",
    "                                    title='Confusion Matrix (Validation Data)',\n",
    "                                    normalize=False)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report, accuracy_score\n",
    "\n",
    "m1_acc = accuracy_score(y_test1, test_predictions1)*100\n",
    "\n",
    "print('Model #1: Validation Accuracy: {0:.2f}%\\n'.format(m1_acc))\n",
    "print(classification_report(y_test1, test_predictions1, target_names=['Not Injured', 'Injured']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Let's build the same Decision Tree Model as above and add the topic distributions for each pitcher"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract Topic Distributions from LDA model for each pitcher\n",
    "col_names = [\"Topic {0}\".format(x) for x in range(0, n_topics)]\n",
    "topic_dist = lda_tf.transform(dtm_tf)\n",
    "topic_df = pd.DataFrame(topic_dist, columns = col_names)\n",
    "\n",
    "# Join topic dataframe with numerical features from Method #1\n",
    "df2 = pd.concat([df1, topic_df], axis=1)\n",
    "print(f'Original Dataframe Shape: {df1.shape}')\n",
    "print(f'New Dataframe Shape: {df2.shape}\\n')\n",
    "df2.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Perform train test split for the new data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.decomposition import LatentDirichletAllocation\n",
    "\n",
    "# Split dataframe into features and labels\n",
    "features = df2.loc[:, df2.columns != 'tjSurgery']\n",
    "labels = df2['tjSurgery']\n",
    "\n",
    "# Split data using 80% to train model and 20% to validate performance\n",
    "X_train2, X_test2, y_train2, y_test2 = train_test_split(features, labels, test_size = 0.2, random_state = seed)\n",
    "\n",
    "# Confirm Shape of Train/Test data\n",
    "print('Shape of Train Features: {}'.format(X_train2.shape))\n",
    "print('Shape of Train Labels:   {}'.format(y_train2.shape))\n",
    "print('Shape of Test Features:  {}'.format(X_test2.shape))\n",
    "print('Shape of Test Labels:    {}'.format(y_test2.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "dtree2 = DecisionTreeClassifier(class_weight='balanced', random_state=seed)\n",
    "dtree2.fit(X_train2, y_train2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Compare Feature Importance for Model #1 (Numerical-only) and Model #2 (Numerical + LDA)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get top_n feature importances from decision tree\n",
    "top_n = 10\n",
    "feat_imp2 = pd.DataFrame({'Importance': dtree2.feature_importances_})    \n",
    "feat_imp2['Feature'] = X_train2.columns\n",
    "feat_imp2.sort_values(by='Importance', ascending=False, inplace=True)\n",
    "feat_imp2 = feat_imp2.iloc[:top_n]\n",
    "\n",
    "# Plot Feature Importance Values\n",
    "plt.figure(figsize=(16,12))\n",
    "\n",
    "# Plot the feature importance for Model #1\n",
    "plt.subplot(2, 1, 1)\n",
    "plt.bar(feat_imp1['Feature'], feat_imp1['Importance'])\n",
    "plt.xticks(rotation=45)\n",
    "plt.title('Model #1: Feature Importance')\n",
    "\n",
    "# Plot the feature importance for Model #2\n",
    "plt.subplot(2, 1, 2)\n",
    "plt.bar(feat_imp2['Feature'], feat_imp2['Importance'])\n",
    "plt.xticks(rotation=45)\n",
    "plt.title('Model #2: Feature Importance')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make predictions with model 2 on validation data\n",
    "test_predictions2 = dtree2.predict(X_test2)\n",
    "\n",
    "plt.figure(figsize=(16,8))\n",
    "\n",
    "# Re-Plot Confusion Matrix on Validation Data for Model #1 (for comparison)\n",
    "plot1 = skplt.metrics.plot_confusion_matrix(y_test1, test_predictions1, \n",
    "                                            figsize=(8,8),\n",
    "                                            title='Model #1: Numerical Features',\n",
    "                                            normalize=False,\n",
    "                                            ax = plt.subplot(1, 2, 1))\n",
    "\n",
    "# Plot Confusion Matrix on Validation Data for Model #2\n",
    "plot2 = skplt.metrics.plot_confusion_matrix(y_test2, test_predictions2, \n",
    "                                            figsize=(8,8),\n",
    "                                            title='Model #2: Numerical + LDA Topics',\n",
    "                                            normalize=False,\n",
    "                                            ax = plt.subplot(1, 2, 2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "m2_acc = accuracy_score(y_test2, test_predictions2)*100\n",
    "\n",
    "target_names = ['Not Injured','Injured']\n",
    "print('Model #2: Overall Accuracy: {0:.2f}%'.format(m2_acc))\n",
    "print('Improvement over Model #1:   {0:.2f}%\\n'.format(m2_acc - m1_acc))\n",
    "print('Model #1 (Numerical Features Only):')\n",
    "print(classification_report(y_test1, test_predictions1, target_names=target_names))\n",
    "print('Model #2 (Numerical Features + LDA Topics):')\n",
    "print(classification_report(y_test2, test_predictions2, target_names=target_names))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Next Steps\n",
    "\n",
    "- Implement technique to identify the optimal number of topics for LDA model and assess impact\n",
    "- Try different sampling techniques (up, down, SMOTE, etc.) and understand performance impacts\n",
    "- Download Lahman database master table and identify which players are still active. Use the Master table to find the pitchers that ended their career with no injury.\n",
    "- "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
